{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lsaq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import GPUtil\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaAttention,\n",
    "    LlamaDecoderLayer,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaMLP,\n",
    ")\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaAttention\n",
    "import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_id:5; gpu_memory:49340.0\n"
     ]
    }
   ],
   "source": [
    "gpus = GPUtil.getGPUs()\n",
    "free_memory = []\n",
    "\n",
    "for gpu in gpus:\n",
    "    free_memory.append(gpu.memoryFree)\n",
    "\n",
    "memory_sort = sorted(range(len(free_memory)), key=lambda i: free_memory[i])\n",
    "\n",
    "gpu_id = memory_sort[-1]\n",
    "gpu_memory = free_memory[memory_sort[-1]]\n",
    "\n",
    "print(f'gpu_id:{gpu_id}; gpu_memory:{gpu_memory}')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/data/zbr/LLMs/Llama-2-7b-hf\"\n",
    "# model_name = \"/data/llms/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers to quant:[27, 26, 25, 28, 24, 29, 23, 21, 22, 30, 19, 20, 18, 17, 16, 11]\n"
     ]
    }
   ],
   "source": [
    "### llama_2 layer importance: [27, 26, 25, 28, 24, 29, 23, 21, 22, 30, 19, 20, 18, 17, 16, 11, 12, 13, 14, 15, 10, 9, 8, 7, 6, 5, 3, 2, 4, 1, 31, 0]\n",
    "### llama_3 layer importance: [23, 24, 25, 27, 26, 22, 28, 21, 20, 19, 18, 29, 17, 11, 13, 16, 10, 12, 9, 15, 8, 14, 3, 2, 7, 6, 4, 5, 30, 31, 1, 0]\n",
    "Qwen_LI = [3, 7, 12, 2, 17, 11, 13, 16, 15, 4, 18, 20, 8, 21, 10, 9, 14, 32, 1, 33, 19, 23, 34, 31, 22, 30, 29, 24, 26, 5, 25, 28, 27, 6, 0, 35]\n",
    "llama_LI = [27, 26, 25, 28, 24, 29, 23, 21, 22, 30, 19, 20, 18, 17, 16, 11, 12, 13, 14, 15, 10, 9, 8, 7, 6, 5, 3, 2, 4, 1, 31, 0]\n",
    "\n",
    "layer_to_quant = llama_LI[:16]\n",
    "# layer_to_quant = Qwen_LI[:18]\n",
    "\n",
    "mlp_quant = [f'layers.{item}.mlp' for item in layer_to_quant]\n",
    "self_attn_quant = [f'layers.{item}.self_attn' for item in layer_to_quant]\n",
    "\n",
    "print(f'layers to quant:{layer_to_quant}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_to_uint8(quant_x, in_f, out_f):\n",
    "    # out_f 为行，in_f 为列\n",
    "    i = 0\n",
    "    rounds = int(out_f/2)\n",
    "\n",
    "    new_weight = torch.zeros(rounds, in_f, dtype=torch.uint8)\n",
    "\n",
    "    while i < rounds:\n",
    "\n",
    "        row0 = quant_x[i] \n",
    "        row1 = quant_x[i + rounds] \n",
    "\n",
    "        packed_row0_row1 = ((row0 + 8).to(torch.uint8) << 4) | (row1 + 8).to(torch.uint8)  # 第一列（高4位）+第三列（低4位）\n",
    "\n",
    "        new_weight[i] = packed_row0_row1\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return new_weight\n",
    "\n",
    "def unpack_to_int8(packed_x, in_f, out_f):\n",
    "\n",
    "    i = 0\n",
    "    rounds = int(out_f/2)\n",
    "\n",
    "    new_weight = torch.zeros(out_f, in_f, dtype=torch.int8)\n",
    "\n",
    "    while i < rounds:\n",
    "\n",
    "        packed_row0_row1 = packed_x[i]\n",
    "        row0 = ((packed_row0_row1 >> 4) & 0x0F).to(torch.int8) - 8  # 高4位→原第一列\n",
    "        row1 = (packed_row0_row1 & 0x0F).to(torch.int8) - 8        # 低4位→原第三列\n",
    "\n",
    "        new_weight[i] = row0\n",
    "        new_weight[i + rounds] = row1\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, name_in, in_features, out_features, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    # w.div_(scales).round_().mul_(scales)\n",
    "    w.div_(scales).round_()\n",
    "    w = w.to(torch.int8)\n",
    "    scales_dict[name_in] = scales\n",
    "    if n_bits == 4:\n",
    "        w = pack_to_uint8(w, in_features, out_features)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, name_in, in_features, out_features, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    # w.div_(scales).round_().mul_(scales)\n",
    "    w.div_(scales).round_()\n",
    "    w = w.to(torch.int8)\n",
    "    scales_dict[name_in] = scales\n",
    "    if n_bits == 4:\n",
    "        w = pack_to_uint8(w, in_features, out_features)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSAQLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name_in,\n",
    "        bit_width,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name_in = name_in\n",
    "        self.bit_width = bit_width\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(LSAQLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        if self.bit_width == 4:\n",
    "            weight = unpack_to_int8(self.weight, self.in_features, self.out_features)\n",
    "            weight = weight.to('cuda')\n",
    "            weight = weight.mul(pth_scales[self.name_in])\n",
    "        else:\n",
    "            weight = self.weight.mul(pth_scales[self.name_in])\n",
    "            \n",
    "        y = torch.functional.F.linear(x, weight, self.bias)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        name_in, bit, module, weight_quant=\"per_channel\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = LSAQLinear(\n",
    "            name_in,\n",
    "            bit,\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(module.weight, name_in, module.in_features, module.out_features, bit)\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(module.weight, name_in, module.in_features, module.out_features, bit)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LSAQLinear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_qwen_like(\n",
    "    model, mlp_quant, self_attn_quant, low_bit, weight_quant=\"per_channel\", quantize_bmm_input=False\n",
    "):\n",
    "    from transformers.models.qwen3.modeling_qwen3 import (\n",
    "        Qwen3Attention,\n",
    "        Qwen3MLP,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, Qwen3MLP):\n",
    "            print(name)\n",
    "            if low_bit == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if name in mlp_quant:\n",
    "                    bit = low_bit\n",
    "                    print(f'{bit} bit quant')\n",
    "                else:\n",
    "                    if low_bit == 4:\n",
    "                        bit = 8\n",
    "                        print(f'{bit} bit quant')\n",
    "                    elif low_bit == 8:\n",
    "                        continue\n",
    "            name_in = name + '.gate_proj'\n",
    "            m.gate_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.gate_proj, weight_quant=weight_quant\n",
    "            )\n",
    "            name_in = name + '.up_proj'\n",
    "            m.up_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.up_proj, weight_quant=weight_quant\n",
    "            )\n",
    "            name_in = name + '.down_proj'\n",
    "            m.down_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.down_proj, weight_quant=weight_quant\n",
    "            )\n",
    "        elif isinstance(m, Qwen3Attention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            if low_bit == 0:\n",
    "                    continue\n",
    "            else:\n",
    "                if name in self_attn_quant:\n",
    "                    bit = low_bit\n",
    "                else:\n",
    "                    if low_bit == 4:\n",
    "                        bit = 8\n",
    "                    elif low_bit == 8:\n",
    "                        continue\n",
    "            name_in = name + '.q_proj'\n",
    "            m.q_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit,  \n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.k_proj'\n",
    "            m.k_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, \n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.v_proj'\n",
    "            m.v_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, \n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.o_proj'\n",
    "            m.o_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, m.o_proj, weight_quant=weight_quant\n",
    "            )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_llama_like(\n",
    "    model, mlp_quant, self_attn_quant, low_bit, weight_quant=\"per_channel\", quantize_bmm_input=False\n",
    "):\n",
    "    from transformers.models.llama.modeling_llama import (\n",
    "        LlamaAttention,\n",
    "        LlamaMLP,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, LlamaMLP):\n",
    "            print(name)\n",
    "            if low_bit == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if name in mlp_quant:\n",
    "                    bit = low_bit\n",
    "                    print(f'{bit} bit quant')\n",
    "                else:\n",
    "                    if low_bit == 4:\n",
    "                        bit = 8\n",
    "                        print(f'{bit} bit quant')\n",
    "                    elif low_bit == 8:\n",
    "                        continue\n",
    "            name_in = name + '.gate_proj'\n",
    "            m.gate_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.gate_proj, weight_quant=weight_quant\n",
    "            )\n",
    "            name_in = name + '.up_proj'\n",
    "            m.up_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.up_proj, weight_quant=weight_quant\n",
    "            )\n",
    "            name_in = name + '.down_proj'\n",
    "            m.down_proj = LSAQLinear.from_float(\n",
    "                name_in, bit, m.down_proj, weight_quant=weight_quant\n",
    "            )\n",
    "        elif isinstance(m, LlamaAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            if low_bit == 0:\n",
    "                    continue\n",
    "            else:\n",
    "                if name in self_attn_quant:\n",
    "                    bit = low_bit\n",
    "                else:\n",
    "                    if low_bit == 4:\n",
    "                        bit = 8\n",
    "                    elif low_bit == 8:\n",
    "                        continue\n",
    "            name_in = name + '.q_proj'\n",
    "            m.q_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit,  \n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.k_proj'\n",
    "            m.k_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, \n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.v_proj'\n",
    "            m.v_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, \n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            name_in = name + '.o_proj'\n",
    "            m.o_proj = LSAQLinear.from_float(\n",
    "                name_in, \n",
    "                bit, m.o_proj, weight_quant=weight_quant\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.mlp\n",
      "8 bit quant\n",
      "layers.1.mlp\n",
      "8 bit quant\n",
      "layers.2.mlp\n",
      "8 bit quant\n",
      "layers.3.mlp\n",
      "8 bit quant\n",
      "layers.4.mlp\n",
      "8 bit quant\n",
      "layers.5.mlp\n",
      "8 bit quant\n",
      "layers.6.mlp\n",
      "8 bit quant\n",
      "layers.7.mlp\n",
      "8 bit quant\n",
      "layers.8.mlp\n",
      "8 bit quant\n",
      "layers.9.mlp\n",
      "8 bit quant\n",
      "layers.10.mlp\n",
      "8 bit quant\n",
      "layers.11.mlp\n",
      "4 bit quant\n",
      "layers.12.mlp\n",
      "8 bit quant\n",
      "layers.13.mlp\n",
      "8 bit quant\n",
      "layers.14.mlp\n",
      "8 bit quant\n",
      "layers.15.mlp\n",
      "8 bit quant\n",
      "layers.16.mlp\n",
      "4 bit quant\n",
      "layers.17.mlp\n",
      "4 bit quant\n",
      "layers.18.mlp\n",
      "4 bit quant\n",
      "layers.19.mlp\n",
      "4 bit quant\n",
      "layers.20.mlp\n",
      "4 bit quant\n",
      "layers.21.mlp\n",
      "4 bit quant\n",
      "layers.22.mlp\n",
      "4 bit quant\n",
      "layers.23.mlp\n",
      "4 bit quant\n",
      "layers.24.mlp\n",
      "4 bit quant\n",
      "layers.25.mlp\n",
      "4 bit quant\n",
      "layers.26.mlp\n",
      "4 bit quant\n",
      "layers.27.mlp\n",
      "4 bit quant\n",
      "layers.28.mlp\n",
      "4 bit quant\n",
      "layers.29.mlp\n",
      "4 bit quant\n",
      "layers.30.mlp\n",
      "4 bit quant\n",
      "layers.31.mlp\n",
      "8 bit quant\n"
     ]
    }
   ],
   "source": [
    "model_aqi = quantize_llama_like(model, mlp_quant, self_attn_quant, 4)\n",
    "# model_aqi = quantize_qwen_like(model, mlp_quant, self_attn_quant, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scales_dict, 'quantized_weight.pth')\n",
    "pth_scales = torch.load('quantized_weight.pth')\n",
    "model_aqi.cuda()\n",
    "for key in pth_scales.keys():\n",
    "    pth_scales[key] = pth_scales[key].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the capital city of America?\n",
      " hopefully you know the answer is Washington D.C\n",
      "speed:0.01token/s max memory:5354.14M\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "# model_aqi = model\n",
    "# model_aqi.cuda()\n",
    "\n",
    "prompt = \"Where is the capital city of America\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_aqi.device)\n",
    "\n",
    "# Generate\n",
    "start_time = time.time()\n",
    "generate_ids = model_aqi.generate(inputs.input_ids, max_length=20)\n",
    "end_time = time.time()\n",
    "speed = len(generate_ids[0])/(end_time-start_time)\n",
    "\n",
    "print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "print(f\"speed:{speed:.2f}token/s max memory:{torch.cuda.max_memory_allocated(model_aqi.device)/ 1024**2:.2f}M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsaq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
